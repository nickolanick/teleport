---
title: Teleport Trusted Clusters
description: How to configure access and trust between two SSH and Kubernetes environments.
h1: Trusted Clusters
---

The design of Trusted Clusters allows Teleport users to connect to compute
infrastructure located behind firewalls without any open TCP ports. The
real-world usage examples of this capability include:

- Managed service providers (MSP) remotely managing the infrastructure of their clients.
- Device manufacturers remotely maintaining computing appliances deployed on premises.
- Large cloud software vendors managing multiple data centers using a common proxy.

Here is an example of an MSP using Trusted Clusters to obtain access to client clusters:
![MSP Example](../../../img/trusted-clusters/TrustedClusters-MSP.svg)

This guide will explain how to:

- Add and remove Trusted Clusters using CLI commands.
- Enable/disable trust between clusters.
- Establish permissions mapping between clusters using Teleport roles.

<Details title="Teleport Node Tunneling" scope={["enterprise", "oss"]} scopeOnly opened>

 If you have a large number of devices on different networks, such as managed
 IoT devices, you can configure your Teleport Nodes to connect to your cluster
 via Teleport Node Tunneling. Instead of connecting to the Auth Service
 directly, a Node connects to the Proxy Service, and the Auth Service creates a
 reverse tunnel to the Node.

 Learn more in [Adding Nodes to the Cluster](./adding-nodes.mdx).

</Details>

## How Trusted Clusters work

Teleport can partition compute infrastructure into multiple clusters. A cluster
is a group of Teleport SSH Nodes connected to the cluster's Auth Service, which
acts as a certificate authority (CA) for all users and nodes in the cluster.

To retrieve an SSH certificate, users must authenticate with a cluster through a
Proxy Service. If users want to connect to Nodes belonging to different
clusters, they would normally have to use different `--proxy` flags for each
cluster. This is not always convenient.

**Leaf clusters** allow Teleport administrators to connect multiple clusters and
establish trust between them. Trusted Clusters allow users of one cluster, the
**root cluster**, to seamlessly SSH into the Nodes of another cluster without having
to "hop" between proxy servers. Moreover, users don't even need to have a direct
connection to other clusters' Proxy Service.

(!docs/pages/includes/permission-warning.mdx!)

The user experience looks like this:

<Tabs>
<TabItem scope={["oss", "enterprise"]} label="Self-Hosted">

```code
# Log in using the root "root" cluster credentials:
$ tsh login --proxy=root.example.com

# SSH into some host inside the "root" cluster:
$ tsh ssh host

# SSH into the host located in another cluster called "leaf"
# The connection is established through root.example.com:
$ tsh ssh --cluster=leaf host

# See what other clusters are available
$ tsh clusters
```

</TabItem>
<TabItem scope={["cloud"]} label="Teleport Cloud">

```code
# Log in using the root "root" cluster credentials:
$ tsh login --proxy=mytenant.teleport.sh

# SSH into some host inside the "root" cluster:
$ tsh ssh host

# SSH into the host located in another cluster called "leaf"
# The connection is established through root.example.com:
$ tsh ssh --cluster=leaf host

# See what other clusters are available
$ tsh clusters
```

</TabItem>
</Tabs>

Once a connection has been established, it's easy to switch from the root cluster.
![Teleport Cluster Page](../../../img/trusted-clusters/teleport-trusted-cluster.png)

Let's take a look at how a connection is established between the "root" cluster
and the "leaf" cluster:

![Tunnels](../../../img/tunnel.svg)

This setup works as follows: the "leaf" creates an outbound reverse SSH tunnel
to "root" and keeps the tunnel open. When a user tries to connect to a Node
inside "leaf" using the root's Proxy Service, the reverse tunnel is used to establish
this connection shown as the green line above.

**Accessibility only works in one direction.** The "leaf" cluster allows users
from "root" to access its Nodes, but users in the "leaf" cluster cannot access
the "root" cluster.


<Admonition
  type="tip"
  title="Load Balancers"
>

  The scheme above also works even if the "root" cluster uses multiple proxies
  behind a load balancer (LB) or a DNS entry with multiple values. This works by
  "leaf" establishing a tunnel to *every* proxy in "root".
  
  This requires that an LB use a round-robin or a similar balancing algorithm.
  Do not use sticky load balancing algorithms (a.k.a. "session affinity" or
  "sticky sessions") with Teleport Proxies.

</Admonition>

## Join Tokens

Let's start with a diagram of how a connection between two clusters is established:

![Tunnels](../../../img/trusted-clusters/TrustedClusters-Simple.svg)

The first step in establishing a secure tunnel between two clusters is for the
*leaf* cluster "leaf" to connect to the *root* cluster "root". When this happens
for *the first time*, clusters know nothing about each other, thus a shared
secret needs to exist for "root" to accept the connection from "leaf".

This shared secret is called a "join token". 

Before following these instructions, you should make sure that you can connect
to Teleport.

(!docs/pages/includes/tctl.mdx!)

<Tabs>
<TabItem scope={["oss", "enterprise"]} label="Self-Hosted">

There are two ways to create join tokens: to statically define them in a
configuration file or to create them on the fly using the `tctl` tool.

<Admonition
  type="tip"
  title="Important"
>

  It's important to note that join tokens are only used to establish a
  connection for the first time. Clusters will exchange certificates and
  won't use tokens to re-establish their connection afterward.

</Admonition>

### Static join tokens

To create a static join token, update the configuration file on the "root"
cluster to look like this:

```yaml
# fragment of /etc/teleport.yaml:
auth_service:
  enabled: true
  tokens:
  # If using static tokens we recommend using tools like `pwgen -s 32`
  # to generate sufficiently random tokens of 32+ byte length
  - trusted_cluster:mk9JgEVqsgz6pSsHf4kJPAHdVDVtpuE0
```

This token can be used an unlimited number of times.

### Dynamic Join Tokens

Creating a token dynamically with a CLI tool offers the advantage of applying a
time-to-live (TTL) interval on it, i.e. it will be impossible to re-use the
token after a specified time.

To create a token using the CLI tool, execute this command on the Auth Server
of cluster "root":

```code
# Generates a Trusted Cluster token to allow an inbound connection from a leaf cluster:
$ tctl tokens add --type=trusted_cluster --ttl=5m
# Example output:
# The cluster invite token: (=presets.tokens.first=)
# This token will expire in 5 minutes

# Generates a Trusted Cluster token with labels.
# Every cluster joined using this token will inherit env:prod labels.
$ tctl tokens add --type=trusted_cluster --labels=env=prod

# You can also list the outstanding non-expired tokens:
$ tctl tokens ls

# ... or delete/revoke an invitation:
$ tctl tokens rm (=presets.tokens.first=)
```
The token created above can be used multiple times and has
an expiration time of 5 minutes.

<Admonition title="Security implications" type="warning">

Consider the security implications when deciding which token method to use.
Short-lived tokens decrease the window for an attack but will require any
automation which uses these tokens to refresh them regularly.

</Admonition>
</TabItem>
<TabItem scope={["cloud"]} label="Teleport Cloud">

You can create a join token on the fly using the `tctl` tool.

<Admonition
  type="tip"
  title="Important"
>

  It's important to note that join tokens are only used to establish a
  connection for the first time. Clusters will exchange certificates and
  won't use tokens to re-establish their connection afterward.

</Admonition>

To create a token using the CLI tool, execute these commands on your development
machine:

```code
# Generates a Trusted Cluster token to allow an inbound connection from a leaf cluster:
$ tctl tokens add --type=trusted_cluster --ttl=5m
# Example output:
# The cluster invite token: (=presets.tokens.first=)
# This token will expire in 5 minutes

# Generates a Trusted Cluster token with labels.
# Every cluster joined using this token will inherit env:prod labels.
$ tctl tokens add --type=trusted_cluster --labels=env=prod

# You can also list the outstanding non-expired tokens:
$ tctl tokens ls

# ... or delete/revoke an invitation:
$ tctl tokens rm (=presets.tokens.first=)
```
The token created above can be used multiple times and has
an expiration time of 5 minutes.

</TabItem>
</Tabs>


Users of Teleport will recognize that this is the same way you would add any
Node to a cluster. 

Now, the administrator of the leaf cluster must create the following
resource file:

```yaml
# cluster.yaml
kind: trusted_cluster
version: v2
metadata:
  # The Trusted Cluster name MUST match the 'cluster_name' setting of the
  # root cluster
  name: root
spec:
  # This field allows to create tunnels that are disabled, but can be enabled later.
  enabled: true
  # The token expected by the "root" cluster:
  token: ba4825847f0378bcdfe18113c4998498
  # The address in 'host:port' form of the reverse tunnel listening port on the
  # "root" proxy server:
  tunnel_addr: root.example.com:3024
  # The address in 'host:port' form of the web listening port on the
  # "root" proxy server:
  web_proxy_addr: root.example.com:443
  # The role mapping allows to map user roles from one cluster to another
  # (enterprise editions of Teleport only)
  role_map:
    - remote: "admin"    # users who have "admin" role on "root"
      local: ["auditor"] # will be assigned "auditor" role when logging into "leaf"
```

Then, use `tctl create` to add the file:

```code
$ tctl create cluster.yaml
```

At this point, the users of the "root" cluster should be able to see "leaf" in the list of available clusters.

## RBAC

When a leaf cluster establishes trust with a root cluster, it needs a way to
configure which users from "root" should be allowed in and what permissions
should they have. Teleport enables you to limit access to Trusted Clusters by
mapping roles to cluster labels.

Trusted Clusters use role mapping for RBAC because both root and leaf clusters
have their own locally defined roles. When creating a `trusted_cluster`
resource, the administrator of the leaf cluster must define how roles from the
root cluster map to roles on the leaf cluster.

<Notice type="tip">
To update the role map for an existing Trusted Cluster, delete and re-create the cluster with the updated role map.
</Notice>

### Using dynamic resources
We will illustrate the use of dynamic resources to configure Trusted Cluster
RBAC with an example.

Let's make a few assumptions for this example:

- The cluster "root" has two roles: *user* for regular users and *admin* for
  local administrators.
- We want administrators from "root" (but not regular users!) to have restricted
  access to "leaf". We want to deny them access to machines with
  `environment:production` and any Government cluster labeled `customer:gov`.

First, we need to create a special role for `root` users on "leaf":

```yaml
# Save this into root-user-role.yaml on the leaf cluster and execute:
# tctl create root-user-role.yaml
kind: role
version: v5
metadata:
  name: local-admin
spec:
  allow:
    node_labels:
      '*': '*'
    # Cluster labels control what clusters user can connect to. The wildcard ('*') means
    # any cluster. If no role in the role set is using labels and the cluster is not labeled,
    # the cluster labels check is not applied. Otherwise, cluster labels are always enforced.
    # This makes the feature backward-compatible.
    cluster_labels:
      'env': '*'
  deny:
    # Cluster labels control what clusters user can connect to. The wildcard ('*') means
    # any cluster. By default none is set in deny rules to preserve backward compatibility
    cluster_labels:
      'customer': 'gov'
    node_labels:
      'environment': 'production'
```

Now, we need to establish trust between the `admin` role on the root cluster and
the `admin` role on the leaf cluster. This is done by creating a
`trusted_cluster` resource on "leaf" which looks like this:

<Tabs>
<TabItem scope={["oss", "enterprise"]} label="Self-Hosted">
```yaml
# Save this as root-cluster.yaml on the auth server of "leaf" and then execute:
# tctl create root-cluster.yaml
kind: trusted_cluster
version: v1
metadata:
  name: "name-of-root-cluster"
spec:
  enabled: true
  role_map:
    - remote: admin
      # admin <-> admin works for the Open Source Edition. Enterprise users
      # have great control over RBAC.
      local: [admin]
  token: "join-token-from-root"
  tunnel_addr: root.example.com:3024
  web_proxy_addr: root.example.com:3080
```
</TabItem>
<TabItem scope={["cloud"]} label="Teleport Cloud">
```yaml
# Save this as root-cluster.yaml on the auth server of "leaf" and then execute:
# tctl create root-cluster.yaml
kind: trusted_cluster
version: v1
metadata:
  name: "name-of-root-cluster"
spec:
  enabled: true
  role_map:
    - remote: admin
      # admin <-> admin works for the Open Source Edition. Enterprise users
      # have great control over RBAC.
      local: [admin]
  token: "join-token-from-root"
  tunnel_addr: mytenant.teleport.sh:3024
  web_proxy_addr: mytenant.teleport.sh:3080
```
</TabItem>
</Tabs>

What if we wanted to let *any* user from "root" to be allowed to connect to
nodes on "leaf"? In this case, we can use a wildcard `*` in the `role_map` like this:

```yaml
role_map:
  - remote: "*"
    local: [access]
```

```yaml
role_map:
   - remote: 'cluster-*'
     local: [clusteradmin]
```

You can also use regular expressions to map user roles from one cluster to
another. Our regular expression syntax enables you to use capture groups to reference part of an remote role name that matches a regular expression in the corresponding local role:

```yaml
  # In this example, remote users with a remote role called 'remote-one' will be
  # mapped to a local role called 'local-one', and `remote-two` becomes `local-two`, etc:
  - remote: "^remote-(.*)$"
    local: [local-$1]
```

Regular expressions use Google's re2 syntax, which you can read about here:

[Syntax](https://github.com/google/re2/wiki/Syntax)

<Notice type="tip">
Regular expression matching is activated only when the expression starts
with `^` and ends with `$`.
</Notice>

<Details title="Trusted Cluster UI" scopeOnly opened scope={["cloud", "enterprise"]}>
You can easily configure leaf nodes using the Teleport Web UI.

Here is an example of creating trust between a leaf and a root node.
![Tunnels](../../../img/trusted-clusters/setting-up-trust.png)
</Details>

## Updating the Trusted Cluster role map

To update the role map for a Trusted Cluster, first, we'll need to remove the cluster by executing:

```code
$ tctl rm tc/root-cluster
```

When this is complete, we can re-create the cluster by executing:

```code
$ tctl create root-user-updated-role.yaml
```

## Updating cluster labels

Teleport gives administrators of root clusters the ability to control cluster labels.
Allowing leaf clusters to propagate their own labels could create a problem with
rogue clusters updating their labels to bad values.

An administrator of a root cluster can control the labels of a remote cluster or
a leaf cluster using the remote cluster API without any fear of override:

```code
$ tctl get rc

# kind: remote_cluster
# metadata:
#  name: two
# status:
#  connection: online
#  last_heartbeat: "2020-09-14T03:13:59.35518164Z"
# version: v3
```

Using `tctl` to update the labels on the remote/leaf cluster:

```code
$ tctl update rc/two --set-labels=env=prod

# Cluster two has been updated
```

Using `tctl` to confirm that the updated labels have been set:

```code
$ tctl get rc

# kind: remote_cluster
# metadata:
#   labels:
#    env: prod
#  name: two
# status:
#  connection: online
#  last_heartbeat: "2020-09-14T03:13:59.35518164Z"
```

## Using Trusted Clusters

Once Trusted Clusters are set up, an admin from the root cluster can see and
access the leaf cluster:

```code
# Log into the root cluster:
$ tsh --proxy=root.example.com login admin
```

```code
# See the list of available clusters
$ tsh clusters

# Cluster Name   Status
# ------------   ------
# root           online
# leaf           online
```

```code
# See the list of machines (nodes) behind the leaf cluster:
$ tsh ls --cluster=leaf

# Node Name Node ID            Address        Labels
# --------- ------------------ -------------- -----------
# db1.leaf  cf7cc5cd-935e-46f1 10.0.5.2:3022  role=db-leader
# db2.leaf  3879d133-fe81-3212 10.0.5.3:3022  role=db-follower
```

```code
# SSH into any node in "leaf":
$ tsh ssh --cluster=leaf user@db1.leaf
```

<Admonition
  type="tip"
  title="Note"
>
  Trusted Clusters work only one way. In the example above, users from "leaf"
  cannot see or connect to the nodes in "root".
</Admonition>

## Disabling trust

To temporarily disable trust between clusters, i.e. to disconnect the "leaf"
cluster from "root", edit the YAML definition of the `trusted_cluster` resource
and set `enabled` to "false", then update it:

```code
$ tctl create --force cluster.yaml
```

### Remove a leaf cluster relationship from both sides

Once established, to fully remove a trust relationship between two clusters, do
the following:

- Remove the relationship from the leaf cluster: `tctl rm tc/root.example.com` (`tc` = Trusted Cluster)
- Remove the relationship from the root cluster: `tctl rm rc/leaf.example.com` (`rc` = remote cluster)

### Remove a leaf cluster relationship from the root

Remove the relationship from the root cluster: `tctl rm rc/leaf.example.com`.

<Admonition type="note">
  The `leaf.example.com` cluster will continue to try and ping the root cluster,
  but will not be able to connect. To re-establish the Trusted Cluster relationship,
  the Trusted Cluster has to be created again from the leaf cluster.
</Admonition>

### Remove a leaf cluster relationship from the leaf

Remove the relationship from the leaf cluster: `tctl rm tc/root.example.com`.

## Sharing user traits between Trusted Clusters

You can share user SSH logins, Kubernetes users/groups, and database users/names between Trusted Clusters.

Suppose you have a root cluster with a role named `root` and the following
allow rules:

```yaml
logins: ["root"]
kubernetes_groups: ["system:masters"]
kubernetes_users: ["alice"]
db_users: ["postgres"]
db_names: ["dev", "metrics"]
```

When setting up the Trusted Cluster relationship, the leaf cluster can choose
to map this `root` cluster role to its own `admin` role:

```yaml
role_map:
- remote: "root"
  local: ["admin"]
```

The role `admin` of the leaf cluster can now be set up to use the root cluster's
role logins, Kubernetes groups and other traits using the following variables:

```yaml
logins: ["{{internal.logins}}"]
kubernetes_groups: ["{{internal.kubernetes_groups}}"]
kubernetes_users: ["{{internal.kubernetes_users}}"]
db_users: ["{{internal.db_users}}"]
db_names: ["{{internal.db_names}}"]
```

User traits that come from the identity provider (such as OIDC claims or SAML
attributes) are also passed to the leaf clusters and can be access in the role
templates using `external` variable prefix:

```yaml
logins: ["{{internal.logins}}", "{{external.logins_from_okta}}"]
node_labels:
  env: "{{external.env_from_okta}}"
```

## How does it work?

At a first glance, Trusted Clusters in combination with RBAC may seem
complicated. However, it is based on certificate-based SSH authentication
which is fairly easy to reason about.

One can think of an SSH certificate as a "permit" issued and time-stamped by a
certificate authority. A certificate contains four important pieces of data:

- List of allowed Unix logins a user can use. They are called "principals" in
  the certificate.
- Signature of the certificate authority that issued it (the Teleport Auth Service)
- Metadata (certificate extensions): additional data protected by the signature
  above. Teleport uses the metadata to store the list of user roles and SSH
  options like "permit-agent-forwarding".
- The expiration date.

Try executing `tsh status` right after `tsh login` to see all these fields in the
client certificate.

When a user from the root cluster tries to connect to a node inside "leaf", the user's
certificate is presented to the Auth Service of "leaf" and it performs the
following checks:

- Checks that the certificate signature matches one of the Trusted Clusters.
- Tries to find a local role that maps to the list of principals found in the certificate.
- Checks if the local role allows the requested identity (Unix login) to have access.
- Checks that the certificate has not expired.

## Troubleshooting

<Tabs>
<TabItem scope={["oss", "enterprise"]} label="Self-Hosted">
There are three common types of problems Teleport administrators can run into when configuring
trust between two clusters:

- **HTTPS configuration**: when the root cluster uses a self-signed or invalid HTTPS certificate.
- **Connectivity problems**: when a leaf cluster does not show up in the output
  of `tsh clusters` on the root cluster.
- **Access problems**: when users from the root cluster get "access denied" error messages
  trying to connect to nodes on the leaf cluster.

### HTTPS configuration

If the `web_proxy_addr` endpoint of the root cluster uses a self-signed or
invalid HTTPS certificate, you will get an error: "the trusted cluster uses
misconfigured HTTP/TLS certificate". For ease of testing, the `teleport` daemon
on the leaf cluster can be started with the `--insecure` CLI flag to accept
self-signed certificates. Make sure to configure HTTPS properly and remove the
insecure flag for production use.

### Connectivity problems

To troubleshoot connectivity problems, enable verbose output for the Auth
Servers on both clusters. Usually this can be done by adding `--debug` flag to
`teleport start --debug`. You can also do this by updating the configuration
file for both Auth Servers:

```yaml
# Snippet from /etc/teleport.yaml
teleport:
  log:
    output: stderr
    severity: DEBUG
```

On systemd-based distributions, you can watch the log output via:

```code
$ journalctl -fu teleport
```

Most of the time you will find out that either a join token is
mismatched/expired, or the network addresses for `tunnel_addr` or
`web_proxy_addr` cannot be reached due to pre-existing firewall rules or
how your network security groups are configured on AWS.

### Access problems

Troubleshooting access denied messages can be challenging. A Teleport administrator
should check to see the following:

- Which roles a user is assigned on the root cluster when they retrieve their SSH
  certificate via `tsh login`. You can inspect the retrieved certificate with the
  `tsh status` command on the client-side.
- Which roles a user is assigned on the leaf cluster when the role mapping takes
  place. The role mapping result is reflected in the Teleport audit log. By
  default, it is stored in `/var/lib/teleport/log` on the Auth Server of a
  cluster. Check the audit log messages on both clusters to get answers for the
  questions above. 
</TabItem>
<TabItem scope={["cloud"]} label="Teleport Cloud">
Troubleshooting "access denied" messages can be challenging. A Teleport administrator
should check to see the following:

- Which roles a user is assigned on the root cluster when they retrieve their SSH
  certificate via `tsh login`. You can inspect the retrieved certificate with the
  `tsh status` command on the client-side.
- Which roles a user is assigned on the leaf cluster when the role mapping takes
  place. The role mapping result is reflected in the Teleport audit log, which
  you can access via the Teleport Web UI.
</TabItem>
</Tabs>

## Further reading
- Read more about how Trusted Clusters fit into Teleport's overall architecture:
  [Architecture Introduction](../../architecture/overview.mdx).

